{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae90552d-0d4a-4b69-ad0d-7ff41ffdb3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from data_zoo import get_llm_classification_data\n",
    "from model_zoo import get_llm\n",
    "from utils.llm_utils import get_experiment_records, get_recalib_predata\n",
    "\n",
    "def config():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--dataset_name\", type=str, default=\"imdb\")\n",
    "    parser.add_argument(\"--model_name\", type=str, default=\"alpaca7b\")\n",
    "    parser.add_argument(\"--model-path\", type=str, required=True, help=\"Path to the Alpaca7b model.\")\n",
    "    parser.add_argument(\"--output-dir\", type=str, default=\"./outputs\")\n",
    "    parser.add_argument(\"--batch-size\", type=int, default=16)\n",
    "    parser.add_argument(\"-f\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "args = config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5ccdb9-b2a1-4e62-aa6c-2a3687f2cc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_llm(args.model_name, model_path=args.model_path)\n",
    "train_sentences, train_labels, test_sentences, test_labels, ds_params = get_llm_classification_data(args.dataset_name, filter_tokenizer=model.tokenizer)\n",
    "label_choices = list(ds_params[\"inv_label_dict\"].keys())\n",
    "\n",
    "# Offset by 1 to avoid the start token. This holds for Llama family models.\n",
    "label_tokens = [model.tokenizer.encode(c)[1] for c in label_choices]\n",
    "ds_params[\"label_tokens\"] = label_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce63a8ef-2e80-4e90-aa6c-faea9cbdfdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "calib_result_path = os.path.join(args.output_dir, f\"calib_llm_{args.model_name}_{args.dataset_name}.pkl\")\n",
    "test_result_path = os.path.join(args.output_dir, f\"test_llm_{args.model_name}_{args.dataset_name}.pkl\")\n",
    "recalib_info_path = os.path.join(args.output_dir, f\"recalib_llm_{args.model_name}_{args.dataset_name}.pkl\")\n",
    "\n",
    "# get_experiment_records returns a set of stats for the experiment, including the data for completions, predicted probabilities, and atypicality values\n",
    "\n",
    "# If not already computed, compute the records\n",
    "if not os.path.exists(test_result_path):\n",
    "    experiment_records = get_experiment_records(model, test_sentences, test_labels, ds_params, batch_size=args.batch_size)\n",
    "    with open(test_result_path, \"wb\") as f:\n",
    "        pickle.dump(experiment_records, f)\n",
    "\n",
    "if (not os.path.exists(calib_result_path)) and (args.dataset_name != \"imdb\"):\n",
    "    experiment_records = get_experiment_records(model, train_sentences, train_labels, ds_params, batch_size=args.batch_size)\n",
    "    with open(calib_result_path, \"wb\") as f:\n",
    "        pickle.dump(experiment_records, f)\n",
    "\n",
    "    \n",
    "if not os.path.exists(recalib_info_path):\n",
    "    recalib_info = get_recalib_predata(model, ds_params)\n",
    "    pickle.dump(recalib_info, open(recalib_info_path, \"wb\"))\n",
    "\n",
    "# Load the stats.\n",
    "# If IMDB, we do not use the training set due to the contamination (Llama is very likely trained on the IMDB train set).\n",
    "if args.dataset_name != \"imdb\":\n",
    "    calib_records = pickle.load(open(calib_result_path, \"rb\"))\n",
    "test_records = pickle.load(open(test_result_path, \"rb\"))\n",
    "recalib_predata = pickle.load(open(recalib_info_path, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a31cba7-1c69-49c4-8105-3dc842365b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather the logits, labels, and atypicality to use later\n",
    "\n",
    "test_logits = test_records[\"logits\"]\n",
    "test_labels = test_records[\"labels\"]\n",
    "test_atypicality = -np.expand_dims(test_records[\"atypicality_total_logprob\"], axis=1)\n",
    "\n",
    "if args.dataset_name == \"imdb\":\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    test_logits, calib_logits, test_labels, calib_labels, test_atypicality, calib_atypicality = train_test_split(test_logits, test_labels, test_atypicality, test_size=0.5, \n",
    "                                                        stratify=test_labels, random_state=1)\n",
    "\n",
    "else: \n",
    "    calib_logits = calib_records[\"logits\"]\n",
    "    calib_labels = calib_records[\"labels\"]\n",
    "    calib_atypicality = -np.expand_dims(calib_records[\"atypicality_total_logprob\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd3ebba-f833-4460-8048-de54bbc2280d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from calibration import TemperatureScaler, AtypicalityAwareCalibrator\n",
    "from scipy.special import softmax\n",
    "from utils.calibration import compute_calibration \n",
    "from utils.plots import get_fig_records\n",
    "\n",
    "\n",
    "metadata = {\"model\": args.model_name, \"dataset\": args.dataset_name}\n",
    "\n",
    "# Vanilla Model\n",
    "probs = softmax(test_logits, axis=1)\n",
    "accuracy = (np.argmax(test_logits, axis=1) == test_labels).mean()\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Temperature Scaling\n",
    "ts = TemperatureScaler()\n",
    "ts.fit(calib_logits, calib_labels)\n",
    "\n",
    "# AAR\n",
    "aar_calib = AtypicalityAwareCalibrator()\n",
    "aar_calib.fit(calib_logits, calib_atypicality, calib_labels, max_iters=1500)\n",
    "\n",
    "prob_info = {\n",
    "    \"probs\": {\"Uncalibrated\": probs,\n",
    "              \"Temp. Scaling\": ts.predict_proba(test_logits),\n",
    "              \"Atypicality-Aware\": aar_calib.predict_proba(test_logits, test_atypicality)},\n",
    "    \"input_atypicality\": test_atypicality,\n",
    "    \"labels\": test_labels\n",
    "}\n",
    "\n",
    "all_records = get_fig_records(prob_info, N_groups=5, **metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a589b767-3d58-428c-b4c9-3ca783521278",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import FormatStrFormatter\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "data = pd.DataFrame(all_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77c68cc-3797-4566-bde0-8a0990d09a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"paper\", font_scale=2)  # Adjust the font_scale as needed\n",
    "fig, axs = plt.subplots(2, 1, figsize=(5, 4.2))\n",
    "maps = {\"Atypicality-Aware\": \"AAR(Ours)\", \"Temp. Scaling\": \"TS\"}\n",
    "data[\"Recalibration\"] = data[\"Recalibration\"].apply(lambda x: maps[x] if x in maps else x)\n",
    "\n",
    "# Plot ECE vs quantile\n",
    "sns.lineplot(x='quantile', y='ECE', hue='Recalibration', linewidth=2.5, errorbar=('ci', 95), data=data, ax=axs[0], legend=True)\n",
    "barplot = sns.barplot(x='Recalibration', y='Accuracy', hue='Recalibration', dodge=False, errorbar=None, linewidth=2.5, data=data, ax=axs[1])\n",
    "\n",
    "for p in barplot.patches:\n",
    "    barplot.annotate(format(p.get_height(), '.2f'), (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='baseline', color=\"white\", xytext=(0, -20), textcoords='offset points')\n",
    "\n",
    "# Format legends and axes\n",
    "handles, labels = axs[0].get_legend_handles_labels()\n",
    "axs[0].get_legend().remove()\n",
    "axs[0].set_xlabel(\"Input Atypicality Quantile\")\n",
    "axs[1].set_xticklabels([])\n",
    "axs[1].set_xlabel(\"\")\n",
    "for handle in handles:\n",
    "    handle.set_linewidth(6)  # Set the desired line width\n",
    "\n",
    "fig.legend(handles=handles, labels=labels, fontsize=15, loc=\"upper center\", bbox_to_anchor=(0.5, 0.1), fancybox=True, shadow=True, ncol=len(labels))\n",
    "fig.tight_layout()\n",
    "fig.savefig(os.path.join(args.output_dir, f\"{args.dataset_name}_{args.model_name}_llmfigure.pdf\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f425a9ec-b93b-4835-82cf-01f981a66b62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3554a99f-cbef-48a3-905f-3018541cd212",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
